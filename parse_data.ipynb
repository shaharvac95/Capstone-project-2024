{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HSqJA1wd4Ga0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.sparse import spdiags, csr_matrix, find, issparse, coo_matrix\n",
        "from scipy.sparse.linalg import spilu\n",
        "from scipy.io import savemat\n",
        "from sklearn.metrics import pairwise_distances\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Numpy"
      ],
      "metadata": {
        "id": "gHC8zFeRCkab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def supernode(A, c):\n",
        "    edges = np.zeros(len(c), dtype=int)\n",
        "    for i, current_c in enumerate(c):\n",
        "        F = A.copy()\n",
        "        n = F.shape[0]\n",
        "        m = len(current_c)\n",
        "        E = np.zeros((m, n), dtype=int)\n",
        "        prev = 0\n",
        "        for j in range(m):\n",
        "            later = current_c[j] + 1\n",
        "            for k in range(prev, later):\n",
        "                E[j, :] += F[k, :]\n",
        "            prev = later\n",
        "        F = np.zeros((m, m), dtype=int)\n",
        "        prev = 0\n",
        "        for j in range(m):\n",
        "            later = current_c[j] + 1\n",
        "            for k in range(prev, later):\n",
        "                F[:, j] += E[:, k]\n",
        "            prev = later\n",
        "        F = F != 0\n",
        "        edges[i] = np.sum(F)\n",
        "    return edges\n",
        "\n",
        "\n",
        "def remove_zero_row(A, n):\n",
        "    # Check if any columns are zero columns\n",
        "    B = np.sum(A, axis=0) == 0\n",
        "    indices = [i for i in range(n) if not B[i]]\n",
        "    A = A[indices][:, indices]\n",
        "    return A\n",
        "\n",
        "\n",
        "def AMG(fW, beta, NS):\n",
        "    def fine2coarse(W, beta):\n",
        "        n = W.shape[0]\n",
        "        nW = W.multiply(1.0 / np.sum(W, axis=1))\n",
        "        # Select coarse nodes (using ChooseCoarseGreedy_mex)\n",
        "        c = ChooseCoarseGreedy_mex(nW, np.random.permutation(n), beta)\n",
        "\n",
        "        # Compute the interp matrix\n",
        "        ci = np.where(c)[0]\n",
        "        P = W[:, ci]\n",
        "        P = P.multiply(1.0 / np.sum(P, axis=1))\n",
        "\n",
        "        # Make sure coarse points are directly connected to their fine counterparts\n",
        "        jj, ii, pji = find(P.transpose())\n",
        "        # jj, ii, pji = coo_matrix(P.transpose())\n",
        "        sel = ~c[ii]\n",
        "        # P = csr_matrix((np.concatenate((pji[sel], np.ones(sum(c)))), (np.concatenate((jj[sel], np.arange(sum(c)))), np.concatenate((ii[sel], ci)))), shape=(len(c), len(ci)))\n",
        "\n",
        "        # Define mycat function\n",
        "        def mycat(x, y):\n",
        "            return np.concatenate((np.ravel(x), np.ravel(y)))\n",
        "\n",
        "        # Create sparse matrix P\n",
        "        rows = mycat(jj[sel], np.arange(np.sum(c)))\n",
        "        cols = mycat(ii[sel], ci)\n",
        "        values = mycat(pji[sel], np.ones(np.sum(c)))\n",
        "\n",
        "        # Calculate the size of the sparse matrix P\n",
        "        num_rows = np.max(rows) + 1\n",
        "        num_cols = np.max(cols) + 1\n",
        "\n",
        "        # Create a sparse matrix in CSR format\n",
        "        P = csr_matrix((values, (rows, cols)), shape=(num_rows, num_cols)).transpose()\n",
        "\n",
        "        return c, P\n",
        "    n = fW.shape[0]\n",
        "\n",
        "    P = [None] * NS\n",
        "    c = [None] * NS\n",
        "    W = [None] * NS\n",
        "\n",
        "    # Make sure diagonal is zero\n",
        "    W[0] = (fW - spdiags(fW.diagonal(), 0, n, n))\n",
        "    fine = np.arange(n)\n",
        "\n",
        "    for si in range(NS):\n",
        "        tmp_c, P[si] = fine2coarse(W[si], beta)\n",
        "        c[si] = fine[tmp_c]\n",
        "\n",
        "        if si < NS - 1:\n",
        "            W[si + 1] = csr_matrix(P[si].transpose().dot(W[si]).dot(P[si]).transpose()).transpose()\n",
        "\n",
        "            W[si + 1] = W[si + 1] - spdiags(W[si + 1].diagonal(), [0], W[si + 1].shape[0], W[si + 1].shape[1], format='csc')\n",
        "            fine = c[si]\n",
        "\n",
        "    # Restriction matrices\n",
        "    def spmtimesd(x, weights):\n",
        "        return csr_matrix(x.transpose() * weights)\n",
        "\n",
        "    R = [None] * NS\n",
        "    # Apply the function to each element of P using list comprehension\n",
        "    # R = [spmtimesd(x, 1. / np.sum(x, axis=1)) for x in P]\n",
        "    for i, x in enumerate(P):\n",
        "        x_transpose = x.transpose()\n",
        "        column_sum = np.sum(x, axis=0)\n",
        "        inverse_sum = 1.0 / column_sum\n",
        "        result = x_transpose.multiply(inverse_sum.transpose())\n",
        "        result_sparse = csr_matrix(result.transpose()).transpose()\n",
        "        R[i] = result_sparse\n",
        "    # R = [csr_matrix((x.transpose()).multiply(1.0 / np.sum(x, axis=0))) for x in P]\n",
        "    return P, R, W, c\n",
        "\n",
        "\n",
        "def ChooseCoarseGreedy_mex(nC, ord, beta):\n",
        "    n = nC.shape[0]\n",
        "\n",
        "    # allocate space for sum_jc\n",
        "    sum_jc = np.zeros(n)\n",
        "\n",
        "    # allocate space for indicator vector c\n",
        "    c = np.zeros(n, dtype=bool)\n",
        "\n",
        "    for current in ord:\n",
        "        if sum_jc[current] <= beta:\n",
        "            # add current to coarse and update sum_jc accordingly\n",
        "            c[current] = True\n",
        "            sum_jc = sum_jc + nC.getcol(current).toarray().flatten()\n",
        "    return c\n",
        "\n",
        "\n",
        "def DBSCAN(X, epsilon, MinPts):\n",
        "    C = 0\n",
        "    n = X.shape[0]\n",
        "    IDX = np.zeros(n, dtype=int)\n",
        "\n",
        "    # calculate the Euclidean distances between all points in the dataset X\n",
        "    D = pairwise_distances(X, metric='euclidean')\n",
        "\n",
        "    visited = np.zeros(n, dtype=bool)\n",
        "    isnoise = np.zeros(n, dtype=bool)\n",
        "\n",
        "    def RegionQuery(i):\n",
        "        return np.where(D[i, :] <= epsilon)[0]\n",
        "\n",
        "    def ExpandCluster(i, neighbors, C):\n",
        "        IDX[i] = C\n",
        "\n",
        "        k = 0\n",
        "        while k < len(neighbors):\n",
        "            j = neighbors[k]\n",
        "\n",
        "            if not visited[j]:\n",
        "                visited[j] = True\n",
        "                neighbors2 = RegionQuery(j)\n",
        "                if len(neighbors2) >= MinPts:\n",
        "                    neighbors = np.concatenate((neighbors, neighbors2))\n",
        "            if IDX[j] == 0:\n",
        "                IDX[j] = C\n",
        "\n",
        "            k += 1\n",
        "\n",
        "    for i in range(n):\n",
        "        if not visited[i]:\n",
        "            visited[i] = True\n",
        "\n",
        "            Neighbors = RegionQuery(i)\n",
        "            if len(Neighbors) < MinPts:\n",
        "                # X[i, :] is NOISE\n",
        "                isnoise[i] = True\n",
        "            else:\n",
        "                C += 1\n",
        "                ExpandCluster(i, Neighbors, C)\n",
        "\n",
        "    return IDX, isnoise\n",
        "\n",
        "\n",
        "def remove_lone_nodes(a):\n",
        "    \"\"\"Renumbers nodeIDs in order to minimize and optimize the original edges file\"\"\"\n",
        "    flat_values = np.unique(np.sort(a.flatten()))\n",
        "    x = {value: i for i, value in enumerate(flat_values)}\n",
        "    for key, value in x.items():\n",
        "        a[a == key] = value\n",
        "    return a\n",
        "\n",
        "def zero_elements(A, percentage):\n",
        "    # Get the indices of elements that are 1\n",
        "    one_indices = np.where(A == 1)\n",
        "\n",
        "    # Calculate the number of elements to zero\n",
        "    num_to_zero = int(len(one_indices[0]) * percentage / 100)\n",
        "\n",
        "    # Randomly choose indices to zero\n",
        "    zero_indices = np.random.choice(len(one_indices[0]), num_to_zero, replace=False)\n",
        "\n",
        "    # Zero the elements at these indices\n",
        "    A[one_indices[0][zero_indices], one_indices[1][zero_indices]] = 0\n",
        "\n",
        "    return A\n",
        "\n",
        "def remove_random_rows(a, percentage):\n",
        "    rows_to_remove = int(len(a) * percentage / 100)\n",
        "    indices_to_remove = np.random.choice(a.shape[0], size=rows_to_remove, replace=False)\n",
        "\n",
        "    # Remove the selected rows\n",
        "    a_reduced = np.delete(a, indices_to_remove, axis=0)\n",
        "\n",
        "    return a_reduced\n",
        "\n",
        "\n",
        "#TODO: need to check that the input matrix is not affected by the operations in this function\n",
        "def drop_elements_wrap(arr, percentage):\n",
        "    # Calculate the number of elements to drop\n",
        "    num_to_drop = int(len(arr) * percentage / 100)\n",
        "\n",
        "    # Calculate the step size for dropping elements\n",
        "    step_size = len(arr) // num_to_drop\n",
        "\n",
        "    # Create a new array by dropping elements at regular intervals\n",
        "    new_arr = np.delete(arr, np.arange(0, len(arr), step_size))\n",
        "\n",
        "    # If we haven't dropped enough elements due to rounding, drop from the beginning\n",
        "    while len(new_arr) > len(arr) - num_to_drop:\n",
        "        new_arr = np.delete(new_arr, 0)\n",
        "\n",
        "    return new_arr"
      ],
      "metadata": {
        "id": "0kngVxTN4SeW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyTorch"
      ],
      "metadata": {
        "id": "jR6ZcJbgCmZ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def supernode_torch(A, c):\n",
        "    edges = torch.zeros(len(c), dtype=torch.int)\n",
        "    for i, current_c in enumerate(c):\n",
        "        F = A.clone()  # Clone A to avoid modifying the original tensor\n",
        "        n = F.shape[0]\n",
        "        m = len(current_c)\n",
        "        E = torch.zeros((m, n), dtype=torch.int)\n",
        "        prev = 0\n",
        "        for j in range(m):\n",
        "            later = current_c[j] + 1\n",
        "            for k in range(prev, later):\n",
        "                E[j, :] += F[k, :]\n",
        "            prev = later\n",
        "        F = torch.zeros((m, m), dtype=torch.int)\n",
        "        prev = 0\n",
        "        for j in range(m):\n",
        "            later = current_c[j] + 1\n",
        "            for k in range(prev, later):\n",
        "                F[:, j] += E[:, k]\n",
        "            prev = later\n",
        "        F = F != 0\n",
        "        edges[i] = torch.sum(F)\n",
        "    return edges\n",
        "\n",
        "def remove_zero_row_torch(A):\n",
        "    # Sum A across rows (dim=0) and check if any columns sum to zero\n",
        "    # Note: No need to pass `n` anymore, as PyTorch can handle dynamic sizing\n",
        "    col_sums = torch.sum(A, dim=0)\n",
        "    row_sums = torch.sum(A, dim=1)\n",
        "\n",
        "    # Identify non-zero columns and rows\n",
        "    non_zero_cols = col_sums != 0\n",
        "    non_zero_rows = row_sums != 0\n",
        "\n",
        "    # Filter A to remove zero rows and columns\n",
        "    # Note: We use boolean indexing in PyTorch to achieve this\n",
        "    A_filtered = A[non_zero_rows][:, non_zero_cols]\n",
        "\n",
        "    return A_filtered\n",
        "\n",
        "def ChooseCoarseGreedy_mex_torch(nC, ord, beta):\n",
        "    # Assuming nC is a dense tensor here. If nC is originally a sparse matrix,\n",
        "    # you might need to convert it to a dense tensor if it's not too large, or handle it as a sparse tensor.\n",
        "    n = nC.shape[0]\n",
        "\n",
        "    # allocate space for sum_jc\n",
        "    sum_jc = torch.zeros(n)\n",
        "\n",
        "    # allocate space for indicator vector c\n",
        "    c = torch.zeros(n, dtype=torch.bool)\n",
        "\n",
        "    for current in ord:\n",
        "        if sum_jc[current] <= beta:\n",
        "            # add current to coarse and update sum_jc accordingly\n",
        "            c[current] = True\n",
        "            # If nC is a dense tensor, we directly use it; otherwise, convert the needed column to dense\n",
        "            # For sparse nC, you'd use something like: nC[:, current].to_dense().view(-1)\n",
        "            sum_jc += nC[:, current].flatten()\n",
        "    return c"
      ],
      "metadata": {
        "id": "Vos9aWx3CnYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "COHu9hrJ4X_c",
        "outputId": "7afba1c8-e8e8-4dc1-ec3f-0a8cc68aca94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# input_file_path = '/content/drive/MyDrive/Misc-Gan_Project/datasets/cora.txt'\n",
        "\n",
        "# a = np.loadtxt(input_file_path, dtype=int)  # Read the edges from the text file\n",
        "# a = remove_lone_nodes(a)\n",
        "# n = np.max(a) + 1  # Find the maximum element in the entire array and add 1\n",
        "# A = np.zeros((n, n), dtype=int)\n",
        "\n",
        "# for i in range(a.shape[0]):\n",
        "#     # A[a[i, 0] + 1, a[i, 1] + 1] = 1\n",
        "#     A[a[i, 0], a[i, 1]] = 1\n",
        "# A = remove_zero_row(A, n)\n",
        "\n",
        "# # Convert the adjacency matrix to a sparse matrix\n",
        "# A_sparse = csr_matrix(A.transpose()).transpose()\n",
        "# P, R, W, c = AMG(fW=A_sparse, beta=0.2, NS=5)\n",
        "\n",
        "# # Apply AMG clustering\n",
        "# epsilon = 0.5\n",
        "# MinPts = 10\n",
        "# indices = DBSCAN(X=A, epsilon=epsilon, MinPts=MinPts)\n",
        "# # labels = dbscan.fit_predict(pairwise_distances(A_sparse, metric='l2'))\n",
        "\n",
        "# # Extract edges between supernodes\n",
        "# edges = supernode(A, c)\n",
        "# c = [[x + 1 for x in a] for a in c]\n",
        "# data = {'A': A, 'P': P, 'W': W, 'R': R, 'IDX': indices, 'edges': edges, 'c': c}\n",
        "# savemat('/content/drive/MyDrive/Misc-Gan_Project/data/cora.mat', data)"
      ],
      "metadata": {
        "id": "tzXddLFe4vfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_file_path = '/content/drive/MyDrive/Multi-Scale_Graph_Generation_with_GU-net/data/cora.txt'\n",
        "\n",
        "a = np.loadtxt(input_file_path, dtype=int)  # Read the edges from the text file\n",
        "a = remove_lone_nodes(a)\n",
        "n = np.max(a) + 1  # Find the maximum element in the entire array and add 1\n",
        "A = np.zeros((n, n), dtype=int)\n",
        "\n",
        "for i in range(a.shape[0]):\n",
        "    # A[a[i, 0] + 1, a[i, 1] + 1] = 1\n",
        "    A[a[i, 0], a[i, 1]] = 1\n",
        "\n",
        "# Convert the adjacency matrix to a sparse matrix\n",
        "A_sparse = csr_matrix(A.transpose()).transpose()\n",
        "P, R, W, c = AMG(fW=A_sparse, beta=0.2, NS=5)\n",
        "\n",
        "# Apply AMG clustering\n",
        "epsilon = 0.5\n",
        "MinPts = 10\n",
        "indices = DBSCAN(X=A, epsilon=epsilon, MinPts=MinPts)\n",
        "# labels = dbscan.fit_predict(pairwise_distances(A_sparse, metric='l2'))\n",
        "\n",
        "# Extract edges between supernodes\n",
        "edges = supernode(A, c)\n",
        "c = [[x + 1 for x in a] for a in c]\n",
        "data = {'A': A, 'P': P, 'W': W, 'R': R, 'IDX': indices, 'edges': edges, 'c': c}\n",
        "savemat('/content/drive/MyDrive/Multi-Scale_Graph_Generation_with_GU-net/data/cora.mat', data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GqHuBB48BAIo",
        "outputId": "85c9e9a0-b0c3-4c06-a31c-f941694c1e02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-e4c7b792565e>:37: RuntimeWarning: divide by zero encountered in divide\n",
            "  nW = W.multiply(1.0 / np.sum(W, axis=1))\n",
            "<ipython-input-15-e4c7b792565e>:44: RuntimeWarning: divide by zero encountered in divide\n",
            "  P = P.multiply(1.0 / np.sum(P, axis=1))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r /content/data /content/drive/MyDrive/Save"
      ],
      "metadata": {
        "id": "Jbwqs3VXTK8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/drive/MyDrive/Save"
      ],
      "metadata": {
        "id": "CyrvQPslpbZc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}